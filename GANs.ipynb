{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"name":"GANs.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"ua8ZyYHedtg6","colab":{"base_uri":"https://localhost:8080/","height":197},"executionInfo":{"status":"error","timestamp":1630841855115,"user_tz":-120,"elapsed":3725,"user":{"displayName":"filippo boni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANETfj-mXmdZuFDYeLWiiuE2g9PtQxVgmt8iFQw=s64","userId":"07965926532969157009"}},"outputId":"56013f5b-1d59-40c0-ad69-1c74b40bdb22"},"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from tensorflow.keras import layers\n","import matplotlib.image as mpimg\n","import numpy as np\n","import keras\n","import os\n","from collections import defaultdict\n","\n","physical_devices = tf.config.list_physical_devices('GPU') \n","tf.config.experimental.set_memory_growth(physical_devices[0], True)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-a0d7fe229524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mphysical_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_memory_growth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphysical_devices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","metadata":{"id":"d4ZA8yqhdtg8"},"source":["discriminator = keras.Sequential(\n","    [\n","        keras.Input(shape=(28, 28, 1)),\n","        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.GlobalMaxPooling2D(),\n","        layers.Dense(1),\n","    ],\n","    name=\"discriminator\",\n",")\n","discriminator.summary()\n","\n","latent_dim = 128\n","\n","generator = keras.Sequential(\n","    [\n","        keras.Input(shape=(latent_dim,)),\n","        # We want to generate 128 coefficients to reshape into a 7x7x128 map\n","        layers.Dense(7 * 7 * 128),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Reshape((7, 7, 128)),\n","        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n","    ],\n","    name=\"generator\",\n",")\n","generator.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o491Cv_Ydtg-"},"source":["class GAN(keras.Model):\n","    def __init__(self, discriminator, generator, latent_dim=128):\n","        super(GAN, self).__init__()\n","        self.discriminator = discriminator\n","        self.generator = generator\n","        self.latent_dim = latent_dim\n","\n","    def compile(self, d_optimizer, g_optimizer, loss_fn):\n","        super(GAN, self).compile()\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","        self.loss_fn = loss_fn\n","\n","    def\n","    (self, real_images):\n","        if isinstance(real_images, tuple):\n","            real_images = real_images[0]\n","            \n","        batch_size = tf.shape(real_images)[0]\n","        \n","\n","                #####################\n","                ## TRAIN GENERATOR ##\n","                #####################\n","\n","        # Sample random points in the latent space\n","        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","\n","        # Generate images starting from random noise\n","        generated_images = self.generator(random_latent_vectors)\n","        \n","        # Concatenate fake and real images\n","        combined_images = tf.concat([generated_images, real_images], axis=0)\n","\n","        # Concatenate fake and real labels\n","        labels = tf.concat(\n","            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n","        )\n","        \n","        # Add random noise to the labels - important trick! (??)\n","        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n","\n","        # Train the discriminator\n","        with tf.GradientTape() as tape:\n","            # Make discriminator predict the concatenated images\n","            predictions = self.discriminator(combined_images)\n","            # Compute the loss between the prediction of the discriminator and the synthetic labels\n","            d_loss = self.loss_fn(labels, predictions)\n","            \n","        # Compute the gradient of the loss above, wrt the weights of the discriminator\n","        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n","        \n","        # Apply backpropagation to the discriminator\n","        self.d_optimizer.apply_gradients(\n","            zip(grads, self.discriminator.trainable_weights)\n","        )\n","        \n","        \n","                #####################\n","                ## TRAIN GENERATOR ##\n","                #####################\n","                \n","        # Sample random points in the latent space \n","        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","\n","        # Assemble misleading labels that say \"all real images\"\n","        misleading_labels = tf.zeros((batch_size, 1))\n","\n","        # Train the generator\n","        with tf.GradientTape() as tape:\n","            # Make discriminator predict fake images from the generator (starting from random noise)\n","            predictions = self.discriminator(self.generator(random_latent_vectors))\n","            \n","            # Compute the loss between the misleading labels and the prediction of the discriminator \n","            # If the discriminator was fooled and predicted \"REAL\" then the loss will be small, and the generator will be hardly modified by backpropagation\n","            # If the discriminator was NOT fooled and predicted \"FAKE\" then loss will be high, and the generator will be greatly modified by backpropagation\n","            g_loss = self.loss_fn(misleading_labels, predictions) \n","            \n","        # Compute the gradient of the loss above, wrt the weights of the generator \n","        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n","        \n","        # Apply backpropagation to the generator \n","        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n","        \n","        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RGWg0bRndtg_"},"source":["class GANMonitor(keras.callbacks.Callback):\n","    def __init__(self, num_img=3, latent_dim=128):\n","        self.num_img = num_img\n","        self.latent_dim = latent_dim\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n","        generated_images = self.model.generator(random_latent_vectors)\n","        generated_images *= 255\n","        generated_images.numpy()\n","        for i in range(self.num_img):\n","            img = keras.preprocessing.image.array_to_img(generated_images[i])\n","            img.save(\"images/generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T8FAyrwpdtg_"},"source":["# Prepare the dataset. We use both the training & test MNIST digits.\n","batch_size = 64\n","(x_train, _), (x_test, _) = tf.keras.datasets.fashion_mnist.load_data()\n","all_digits = np.concatenate([x_train, x_test])\n","all_digits = all_digits.astype(\"float32\") / 255.0\n","all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n","dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n","dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n","\n","\n","gan = GAN(discriminator=discriminator, generator=generator)\n","gan.compile(\n","    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n","    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n","    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0XUf2WiXdtg_"},"source":["generator.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GCpK6cLzdtg_"},"source":["discriminator.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"ej0ZBgv1dtg_"},"source":["# To limit the execution time, we only train on 100 batches. You can train on\n","# the entire dataset. You will need about 20 epochs to get nice results.\n","gan.fit(dataset, epochs=30, callbacks=[GANMonitor(num_img=3, latent_dim=latent_dim)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5z4uwjr2dthA"},"source":["root = \"images/\"\n","images_dict = defaultdict(lambda: defaultdict())\n","for img in os.listdir(root):\n","    tokens = img.split(\"_\")\n","    img_number = tokens[-2]\n","    epoch = tokens[-1].split(\".\")[0]\n","    images_dict[int(epoch)][int(img_number)] = img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"KHuItHIddthA"},"source":["nepochs = 30\n","imgs = 3\n","\n","fig, ax = plt.subplots(nrows=nepochs, ncols=imgs, figsize=(5, 30))\n","ax = np.ravel(ax)\n","\n","for i in range(nepochs):\n","    for j in range(imgs):\n","        ax[(i*imgs)+j].imshow(mpimg.imread(os.path.join(root, images_dict[i][j])), cmap=\"Greys\")\n","        ax[(i*imgs)+j].axis('off')\n","\n","fig.tight_layout()"],"execution_count":null,"outputs":[]}]}